{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Using TensorFlow backend.\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:516: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:517: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:518: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:519: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:520: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorflow\\python\\framework\\dtypes.py:525: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:541: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint8 = np.dtype([(\"qint8\", np.int8, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:542: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint8 = np.dtype([(\"quint8\", np.uint8, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:543: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint16 = np.dtype([(\"qint16\", np.int16, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:544: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_quint16 = np.dtype([(\"quint16\", np.uint16, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:545: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  _np_qint32 = np.dtype([(\"qint32\", np.int32, 1)])\n",
      "C:\\ProgramData\\Miniconda3\\lib\\site-packages\\tensorboard\\compat\\tensorflow_stub\\dtypes.py:550: FutureWarning: Passing (type, 1) or '1type' as a synonym of type is deprecated; in a future version of numpy, it will be understood as (type, (1,)) / '(1,)type'.\n",
      "  np_resource = np.dtype([(\"resource\", np.ubyte, 1)])\n"
     ]
    }
   ],
   "source": [
    "import cv2\n",
    "import numpy as np \n",
    "import os\n",
    "import random \n",
    "from tqdm import tqdm\n",
    "from keras.layers.convolutional import Conv2D\n",
    "import tensorflow as tf\n",
    "import numpy as np\n",
    "import keras\n",
    "from keras.models import Sequential\n",
    "from keras.layers import MaxPooling2D, Dropout, Flatten, Dense"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def crop_and_save(img, path = \"C:\\\\Users\\\\Pankratz\\\\Desktop\\\\Jupyter_lab2\\\\\"): \n",
    "    \n",
    "    assert img == 'check.jpg' or img == 'cross.jpg', \"значение img должно быть 'check.jpg' или 'cross.jpg' \"\n",
    "    \n",
    "    img_path = os.path.join(path, img)\n",
    "    image = cv2.imread(img_path, cv2.IMREAD_GRAYSCALE) # читаем изображение сразу чёрно-белым\n",
    "    if img == 'cross.jpg': \n",
    "        image = image[20:727, 60:727] # обрезаем ненужные места со скана \n",
    "        max_x = 17\n",
    "        max_y = 18\n",
    "        new_path = 'C:\\\\Users\\\\Pankratz\\\\Desktop\\\\Jupyter_lab2\\\\images\\\\training data\\\\0' # сразу кодируем X как 0\n",
    "    elif img == 'check.jpg': \n",
    "        image = image[25:735, 32:736] # обрезаем ненужные места со скана\n",
    "        max_x = 18 # в листах с крестиками и галочками разное число символов в каждой строке\n",
    "        max_y = 17\n",
    "        new_path = 'C:\\\\Users\\\\Pankratz\\\\Desktop\\\\Jupyter_lab2\\\\images\\\\training data\\\\1' # сразу кодируем V как 1\n",
    "    \n",
    "    size = 32 \n",
    "    final_wide = max_x*size # рассчитываем во сколько раз уменьшим картинку\n",
    "    r = float(final_wide) / image.shape[1]\n",
    "    dim = (final_wide, int(image.shape[0] * r))\n",
    "\n",
    "    resized = cv2.resize(image, dim, interpolation = cv2.INTER_AREA)\n",
    "    # gray = cv2.cvtColor(resized, cv2.COLOR_BGR2GRAY)\n",
    "    blurred = cv2.GaussianBlur(resized, (5, 5), 1) \n",
    "    result = cv2.threshold(blurred, 235, 255, cv2.THRESH_BINARY)[1] #преобразованное изображение\n",
    "    \n",
    "    i = 0\n",
    "    x0 = 0\n",
    "    y0 = 0\n",
    "    count_x = 0\n",
    "    \n",
    "    while count_x < max_x: # цикл по строкам\n",
    "        count_y = 0\n",
    "        x0 = 0\n",
    "        while count_y < max_y: # цикл по столбцам\n",
    "            cropped = result[x0:x0+size, y0:y0+size]\n",
    "            final_path = os.path.join(new_path, \"{0}.jpg\".format(i))\n",
    "            cv2.imwrite(final_path, cropped)\n",
    "            x0 += size\n",
    "            i += 1\n",
    "            count_y +=1\n",
    "        y0 += size\n",
    "        count_x += 1\n",
    "    \n",
    "    return i \n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load_data(train_array, train_answers, my_path = 'C:\\\\Users\\\\Pankratz\\\\Desktop\\\\Jupyter_lab2\\\\images'):\n",
    "    classes = ['0', '1']\n",
    "    directory1 = os.path.join(my_path, 'training data')\n",
    "    for c in classes:\n",
    "        directory_classes = os.path.join(directory1, c)\n",
    "        for pic in tqdm(os.listdir(directory_classes)):\n",
    "            stock = cv2.imread(os.path.join(directory_classes, pic), cv2.IMREAD_GRAYSCALE) \n",
    "            train_array.append(stock / 255)\n",
    "            train_answers.append(c)\n",
    "    \n",
    "    \n",
    "    #images = [cv2.imread(file) for file in glob.glob(\"path/to/files/*.png\")]\n",
    "    \n",
    "    #shfl = list(zip(train_array, train_answers))\n",
    "    #random.shuffle(shfl)\n",
    "    #train_array, train_answers = zip(*shfl)\n",
    "    \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "#models \n",
    "\n",
    "classes = ['0', '1']\n",
    "\n",
    "def one_layer_dense():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(256, activation = 'relu', name = 'input'))\n",
    "    model.add(Dense(len(classes), activation = 'softmax', name = 'output'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "def many_layers_dense():\n",
    "    model = Sequential()\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(1024, activation = 'relu', name = 'input'))\n",
    "    model.add(Dense(512, activation = 'relu'))\n",
    "    model.add(Dropout(0.3))\n",
    "    model.add(Dense(256, activation = 'relu'))\n",
    "    model.add(Dense(len(classes), activation = 'softmax', name = 'output'))\n",
    "    model.compile(loss='categorical_crossentropy', optimizer='adam', metrics=['categorical_accuracy'])\n",
    "    return model\n",
    "\n",
    "def conv_model():\n",
    "    model = Sequential()\n",
    "    model.add(Conv2D(filters = 8, kernel_size = (3,3), activation = 'relu', input_shape = (32, 32, 1)))\n",
    "    model.add(MaxPooling2D((3,3), strides = 1))\n",
    "    model.add(Conv2D(filters = 6, kernel_size = (3,3), activation = 'relu', input_shape = (32, 32, 1)))\n",
    "    model.add(MaxPooling2D((3,3), strides = 1))\n",
    "    model.add(Dropout(0.25))\n",
    "    model.add(Flatten())\n",
    "    model.add(Dense(len(classes), activation = 'softmax', name = 'output'))\n",
    "    model.compile(loss = 'categorical_crossentropy', optimizer = 'adam', metrics = ['categorical_accuracy'])\n",
    "    return model\n",
    "    \n",
    "\n",
    "    \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def predict_answers(model, X_pred, num_pred, conv = False): \n",
    "    \n",
    "    if conv == True:\n",
    "        X_pred = X_pred.reshape(X_pred.shape[0], 32, 32, 1 ) \n",
    "        \n",
    "    arr_pred = np.array(random.choices(X_pred, k = num_pred)) #случайно перемешиваем поданный массив\n",
    "    predictions = model.predict(arr_pred) #генерируем предсказания\n",
    "\n",
    "    for p in range(len(predictions)):\n",
    "        if predictions[p][0] > 0.95: #0.95 - порог уверенности в том что картинка принадлежит к типу\n",
    "            print('cross \\n')\n",
    "        elif predictions[p][1] > 0.95:\n",
    "            print('check \\n')\n",
    "        else:\n",
    "            print('i dunno \\n')\n",
    "        cv2.imshow('w', arr_pred[p]) #\n",
    "        cv2.waitKey(0)\n",
    "        cv2.destroyAllWindows()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|███████████████████████████████████████████████████████████████████████████████| 306/306 [00:02<00:00, 108.62it/s]\n",
      "100%|███████████████████████████████████████████████████████████████████████████████| 306/306 [00:02<00:00, 142.17it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "WARNING:tensorflow:From C:\\ProgramData\\Miniconda3\\lib\\site-packages\\keras\\backend\\tensorflow_backend.py:422: The name tf.global_variables is deprecated. Please use tf.compat.v1.global_variables instead.\n",
      "\n",
      "Train on 408 samples, validate on 102 samples\n",
      "Epoch 1/18\n",
      "408/408 [==============================] - 0s 1ms/step - loss: 0.4227 - categorical_accuracy: 0.8652 - val_loss: 0.0196 - val_categorical_accuracy: 0.9902\n",
      "Epoch 2/18\n",
      "408/408 [==============================] - 0s 563us/step - loss: 0.0267 - categorical_accuracy: 0.9926 - val_loss: 0.0431 - val_categorical_accuracy: 0.9804\n",
      "Epoch 3/18\n",
      "408/408 [==============================] - 0s 568us/step - loss: 0.0168 - categorical_accuracy: 0.9926 - val_loss: 0.0250 - val_categorical_accuracy: 0.9804\n",
      "Epoch 4/18\n",
      "408/408 [==============================] - 0s 570us/step - loss: 0.0146 - categorical_accuracy: 0.9951 - val_loss: 0.0102 - val_categorical_accuracy: 1.0000\n",
      "Epoch 5/18\n",
      "408/408 [==============================] - 0s 547us/step - loss: 0.0089 - categorical_accuracy: 0.9975 - val_loss: 0.0083 - val_categorical_accuracy: 1.0000\n",
      "Epoch 6/18\n",
      "408/408 [==============================] - 0s 529us/step - loss: 0.0051 - categorical_accuracy: 1.0000 - val_loss: 0.0076 - val_categorical_accuracy: 1.0000\n",
      "Epoch 7/18\n",
      "408/408 [==============================] - 0s 543us/step - loss: 0.0038 - categorical_accuracy: 1.0000 - val_loss: 0.0078 - val_categorical_accuracy: 1.0000\n",
      "Epoch 8/18\n",
      "408/408 [==============================] - 0s 575us/step - loss: 0.0040 - categorical_accuracy: 1.0000 - val_loss: 0.0097 - val_categorical_accuracy: 1.0000\n",
      "Epoch 9/18\n",
      "408/408 [==============================] - 0s 547us/step - loss: 0.0028 - categorical_accuracy: 1.0000 - val_loss: 0.0030 - val_categorical_accuracy: 1.0000\n",
      "Epoch 10/18\n",
      "408/408 [==============================] - 0s 543us/step - loss: 0.0016 - categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_categorical_accuracy: 1.0000\n",
      "Epoch 11/18\n",
      "408/408 [==============================] - 0s 579us/step - loss: 0.0015 - categorical_accuracy: 1.0000 - val_loss: 0.0037 - val_categorical_accuracy: 1.0000\n",
      "Epoch 12/18\n",
      "408/408 [==============================] - 0s 579us/step - loss: 0.0012 - categorical_accuracy: 1.0000 - val_loss: 0.0018 - val_categorical_accuracy: 1.0000\n",
      "Epoch 13/18\n",
      "408/408 [==============================] - 0s 532us/step - loss: 0.0011 - categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_categorical_accuracy: 1.0000\n",
      "Epoch 14/18\n",
      "408/408 [==============================] - 0s 595us/step - loss: 9.6490e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0013 - val_categorical_accuracy: 1.0000\n",
      "Epoch 15/18\n",
      "408/408 [==============================] - 0s 562us/step - loss: 8.4606e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0024 - val_categorical_accuracy: 1.0000\n",
      "Epoch 16/18\n",
      "408/408 [==============================] - 0s 523us/step - loss: 7.7217e-04 - categorical_accuracy: 1.0000 - val_loss: 7.2097e-04 - val_categorical_accuracy: 1.0000\n",
      "Epoch 17/18\n",
      "408/408 [==============================] - 0s 661us/step - loss: 8.2461e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0010 - val_categorical_accuracy: 1.0000\n",
      "Epoch 18/18\n",
      "408/408 [==============================] - 0s 563us/step - loss: 7.0926e-04 - categorical_accuracy: 1.0000 - val_loss: 0.0015 - val_categorical_accuracy: 1.0000\n",
      "98/98 [==============================] - 0s 174us/step\n",
      "\n",
      " test loss, test acc: [0.0015334495162408399, 1.0]\n",
      "Предсказания модели: \n",
      "\n",
      "cross \n",
      "\n",
      "cross \n",
      "\n",
      "check \n",
      "\n",
      "cross \n",
      "\n"
     ]
    }
   ],
   "source": [
    "#one layer model\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "a = crop_and_save(\"check.jpg\") \n",
    "b = crop_and_save(\"cross.jpg\") \n",
    "sum = a+b\n",
    "\n",
    "load_data(X_train, y_train)\n",
    "\n",
    "X_train = (np.array(X_train))\n",
    "y_train = np.array(y_train)\n",
    "y_train_encoded = (keras.utils.to_categorical(y_train))\n",
    "#print(X_train.shape, y_train_encoded.shape)\n",
    "#print(y_train_encoded)\n",
    "\n",
    "\n",
    "X_pred1 = X_train[:2] #выделяем немного данных для демонстрации model.predict\n",
    "y_pred1 = y_train_encoded[:2] #с разных концов упорядоченного датасета, чтобы попадались картинки обоих типов\n",
    "X_pred2 = X_train[-2:]\n",
    "y_pred2 = y_train_encoded[-2:] \n",
    "\n",
    "X_test = X_train[2:100] #выделяем примерно шестую часть данных для тестового сета \n",
    "y_test = y_train_encoded[2:100]\n",
    "\n",
    "X_train = X_train[100:-2]\n",
    "y_train_encoded = y_train_encoded[100:-2]\n",
    "\n",
    "model1 = one_layer_dense()\n",
    "model1.fit(X_train, y_train_encoded, validation_split = 0.2, batch_size = 17, epochs = 18)\n",
    "\n",
    "results = model1.evaluate(X_test, y_test, batch_size = 20)\n",
    "print('\\n test loss, test acc: {0}'.format(results))\n",
    "\n",
    "X_pred = np.concatenate((X_pred1, X_pred2))\n",
    "num_pred = 4 #количество делаемых предсказаний\n",
    "# y_test = np.concatenate((y_test1, y_test2))\n",
    "# ответы не нужны для предсказаний\n",
    "print('Предсказания модели: \\n')\n",
    "predict_answers(model1, X_pred, 4) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#many layers model\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "a = crop_and_save(\"check.jpg\") \n",
    "b = crop_and_save(\"cross.jpg\") \n",
    "sum = a+b\n",
    "\n",
    "load_data(X_train, y_train)\n",
    "\n",
    "X_train = (np.array(X_train))\n",
    "y_train = np.array(y_train)\n",
    "y_train_encoded = (keras.utils.to_categorical(y_train))\n",
    "#print(X_train.shape, y_train_encoded.shape)\n",
    "#print(y_train_encoded)\n",
    "\n",
    "\n",
    "X_pred1 = X_train[:2] #выделяем немного данных для демонстрации model.predict\n",
    "y_pred1 = y_train_encoded[:2] #с разных концов упорядоченного датасета, чтобы попадались картинки обоих типов\n",
    "X_pred2 = X_train[-2:]\n",
    "y_pred2 = y_train_encoded[-2:] \n",
    "\n",
    "X_test = X_train[2:100] #выделяем примерно шестую часть данных для тестового сета \n",
    "y_test = y_train_encoded[2:100]\n",
    "\n",
    "X_train = X_train[100:-2]\n",
    "y_train_encoded = y_train_encoded[100:-2]\n",
    "\n",
    "model2 = many_layers_dense()\n",
    "model2.fit(X_train, y_train_encoded, validation_split = 0.2, batch_size = 17, epochs = 18)\n",
    "\n",
    "results = model2.evaluate(X_test, y_test, batch_size = 20)\n",
    "print('\\n test loss, test acc: {0}'.format(results))\n",
    "\n",
    "X_pred = np.concatenate((X_pred1, X_pred2))\n",
    "num_pred = 4 #количество делаемых предсказаний\n",
    "# y_test = np.concatenate((y_test1, y_test2))\n",
    "# ответы не нужны для предсказаний\n",
    "print('Предсказания модели: \\n')\n",
    "predict_answers(model2, X_pred, 4) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#convolutional model\n",
    "\n",
    "import keras\n",
    "\n",
    "\n",
    "X_train = []\n",
    "y_train = []\n",
    "X_test = []\n",
    "y_test = []\n",
    "\n",
    "a = crop_and_save(\"check.jpg\") \n",
    "b = crop_and_save(\"cross.jpg\") \n",
    "sum = a+b\n",
    "\n",
    "load_data(X_train, y_train)\n",
    "\n",
    "X_train = (np.array(X_train))\n",
    "y_train = np.array(y_train)\n",
    "y_train_encoded = (keras.utils.to_categorical(y_train))\n",
    "#print(X_train.shape, y_train_encoded.shape)\n",
    "#print(y_train_encoded)\n",
    "\n",
    "\n",
    "X_pred1 = X_train[:2] #выделяем немного данных для демонстрации model.predict\n",
    "y_pred1 = y_train_encoded[:2] #с разных концов упорядоченного датасета, чтобы попадались картинки обоих типов\n",
    "X_pred2 = X_train[-2:]\n",
    "y_pred2 = y_train_encoded[-2:] \n",
    "\n",
    "X_test = X_train[2:100] #выделяем примерно шестую часть данных для тестового сета \n",
    "y_test = y_train_encoded[2:100]\n",
    "\n",
    "X_train = X_train[100:-2]\n",
    "y_train_encoded = y_train_encoded[100:-2]\n",
    "\n",
    "X_train = X_train.reshape(X_train.shape[0], 32, 32, 1)\n",
    "X_test = X_test.reshape(X_test.shape[0], 32, 32, 1)\n",
    "\n",
    "model3 = conv_model()\n",
    "model3.fit(X_train, y_train_encoded, validation_split = 0.2, batch_size = 17, epochs = 18)\n",
    "\n",
    "results = model3.evaluate(X_test, y_test, batch_size = 20)\n",
    "print('\\n test loss, test acc: {0}'.format(results))\n",
    "\n",
    "X_pred = np.concatenate((X_pred1, X_pred2))\n",
    "num_pred = 4 #количество делаемых предсказаний\n",
    "print('Предсказания модели: \\n')\n",
    "predict_answers(model3, X_pred, 4, conv = True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
